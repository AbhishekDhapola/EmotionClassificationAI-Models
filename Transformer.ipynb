{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from \"merged_training.pkl\"\n",
    "df = pd.read_pickle(\"merged_training.pkl\")\n",
    "\n",
    "# Encode the emotion labels using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['emotions_encoded'] = label_encoder.fit_transform(df['emotions'])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X = df['text']\n",
    "y = df['emotions_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the list of emotion labels for later decoding\n",
    "emotions = list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\Abhishek\\Anaconda\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1553, in compute_loss  *\n        return super().compute_loss(*args, **kwargs)\n    File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss  **\n        return self.compiled_loss(\n    File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 275, in __call__\n        y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n    File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 854, in match_dtype_and_rank\n        if (y_t.dtype.is_floating and y_p.dtype.is_floating) or (\n\n    AttributeError: 'NoneType' object has no attribute 'dtype'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Abhishek\\Job Prep\\Transformer.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Abhishek/Job%20Prep/Transformer.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model_bert\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m), loss\u001b[39m=\u001b[39mmodel_bert\u001b[39m.\u001b[39mcompute_loss, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Abhishek/Job%20Prep/Transformer.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Train the BERT model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Abhishek/Job%20Prep/Transformer.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model_bert\u001b[39m.\u001b[39mfit(train_encodings, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file_6lk0hgf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Abhishek\\Anaconda\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1671\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1668\u001b[0m             y_pred \u001b[39m=\u001b[39m y_pred[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1670\u001b[0m     \u001b[39mif\u001b[39;00m loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1671\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_loss(y, y_pred, sample_weight, regularization_losses\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses)\n\u001b[0;32m   1673\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m   1674\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mminimize(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable_variables, tape\u001b[39m=\u001b[39mtape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file613m0nqm.py:38\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mhasattr\u001b[39m), (ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel, \u001b[39m'\u001b[39m\u001b[39mcompute_loss\u001b[39m\u001b[39m'\u001b[39m), \u001b[39mNone\u001b[39;00m, fscope), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file613m0nqm.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss.<locals>.if_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39msuper\u001b[39m), (), \u001b[39mNone\u001b[39;00m, fscope)\u001b[39m.\u001b[39mcompute_loss, \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(args)), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(kwargs)), fscope)\n\u001b[0;32m     25\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\Abhishek\\Anaconda\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1553, in compute_loss  *\n        return super().compute_loss(*args, **kwargs)\n    File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1185, in compute_loss  **\n        return self.compiled_loss(\n    File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 275, in __call__\n        y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n    File \"C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 854, in match_dtype_and_rank\n        if (y_t.dtype.is_floating and y_p.dtype.is_floating) or (\n\n    AttributeError: 'NoneType' object has no attribute 'dtype'\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the training and test data\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, return_tensors='tf')\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, return_tensors='tf')\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model_bert = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(emotions))\n",
    "\n",
    "# Compile the BERT model\n",
    "model_bert.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=model_bert.compute_loss, metrics=['accuracy'])\n",
    "\n",
    "# Train the BERT model\n",
    "model_bert.fit(train_encodings, y_train, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2084/2084 [==============================] - 67205s 32s/step - loss: 0.3206 - accuracy: 0.8815 - val_loss: 0.1292 - val_accuracy: 0.9327\n",
      "522/522 [==============================] - 1821s 3s/step - loss: 0.1292 - accuracy: 0.9327\n",
      "Loss: 0.1291562020778656, Accuracy: 0.9327096343040466\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from \"merged_training.pkl\"\n",
    "df = pd.read_pickle(\"merged_training.pkl\")\n",
    "\n",
    "# Encode the emotion labels using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['emotions_encoded'] = label_encoder.fit_transform(df['emotions'])\n",
    "\n",
    "# Sample a smaller subset of the data for faster experimentation\n",
    "df = df.sample(frac=0.1, random_state=42)  # Adjust the fraction as needed\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X = df['text']\n",
    "y = df['emotions_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the list of emotion labels for later decoding\n",
    "emotions = list(label_encoder.classes_)\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text data with a reduced maximum sequence length\n",
    "max_length = 64  # Adjust the maximum sequence length\n",
    "X_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, max_length=max_length, return_tensors='tf')\n",
    "X_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, max_length=max_length, return_tensors='tf')\n",
    "\n",
    "# Convert the emotion labels to categorical format\n",
    "num_classes = len(emotions)\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "\n",
    "# Compile the model with a reduced learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)  # Adjust the learning rate\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Train the model with reduced epochs and batch size\n",
    "history = model.fit(\n",
    "    X_train_tokens.data,\n",
    "    y_train,\n",
    "    validation_data=(X_test_tokens.data, y_test),\n",
    "    epochs=1,  # Reduce the number of epochs\n",
    "    batch_size=16  # Reduce the batch size\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_tokens.data, y_test, batch_size=16)  # Use the same reduced batch size\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('emotion_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
