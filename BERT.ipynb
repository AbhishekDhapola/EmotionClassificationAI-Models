{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from \"merged_training.pkl\"\n",
    "df = pd.read_pickle(\"merged_training.pkl\")\n",
    "\n",
    "# Encode the emotion labels using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['emotions_encoded'] = label_encoder.fit_transform(df['emotions'])\n",
    "\n",
    "# Sample a smaller subset of the data for faster experimentation\n",
    "df = df.sample(frac=0.1, random_state=42)  # Adjust the fraction as needed\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X = df['text']\n",
    "y = df['emotions_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the list of emotion labels for later decoding\n",
    "emotions = list(label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT WITH REDUCED EPOCH,BATCH SIZE & LEARNING RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2084/2084 [==============================] - 67205s 32s/step - loss: 0.3206 - accuracy: 0.8815 - val_loss: 0.1292 - val_accuracy: 0.9327\n",
      "522/522 [==============================] - 1821s 3s/step - loss: 0.1292 - accuracy: 0.9327\n",
      "Loss: 0.1291562020778656, Accuracy: 0.9327096343040466\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text data with a reduced maximum sequence length\n",
    "max_length = 64  # Adjust the maximum sequence length\n",
    "X_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, max_length=max_length, return_tensors='tf')\n",
    "X_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, max_length=max_length, return_tensors='tf')\n",
    "\n",
    "# Convert the emotion labels to categorical format\n",
    "num_classes = len(emotions)\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "\n",
    "# Compile the model with a reduced learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)  # Adjust the learning rate\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Train the model with reduced epochs and batch size\n",
    "history = model.fit(\n",
    "    X_train_tokens.data,\n",
    "    y_train,\n",
    "    validation_data=(X_test_tokens.data, y_test),\n",
    "    epochs=1,  # Reduce the number of epochs\n",
    "    batch_size=16  # Reduce the batch size\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_tokens.data, y_test, batch_size=16)  # Use the same reduced batch size\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('emotion_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at emotion_model were not used when initializing TFBertForSequenceClassification: ['dropout_224']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at emotion_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5211/5211 [==============================] - 57997s 11s/step - loss: 0.1184 - accuracy: 0.9375\n",
      "2606/2606 [==============================] - 28884s 11s/step\n",
      "BERT Model Evaluation:\n",
      "Loss: 0.1184\n",
      "Accuracy: 0.9375\n",
      "Precision: 0.9413\n",
      "Recall: 0.9375\n",
      "F1-Score: 0.9374\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.92      0.97      0.94     11339\n",
      "        fear       0.88      0.94      0.91      9376\n",
      "         joy       0.97      0.93      0.95     28247\n",
      "        love       0.79      0.93      0.86      6853\n",
      "     sadness       0.98      0.97      0.97     24504\n",
      "    surprise       0.98      0.65      0.78      3043\n",
      "\n",
      "    accuracy                           0.94     83362\n",
      "   macro avg       0.92      0.90      0.90     83362\n",
      "weighted avg       0.94      0.94      0.94     83362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "# Load the saved BERT model\n",
    "max_length = 64 \n",
    "saved_model = TFBertForSequenceClassification.from_pretrained('emotion_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and preprocess test data to match the input shape used during training\n",
    "X_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, max_length=max_length, return_tensors='tf')\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)  # Adjust the learning rate if necessary\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "saved_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# Evaluate the BERT model on the test dataset\n",
    "loss, accuracy = saved_model.evaluate(X_test_tokens.data, y_test, batch_size=16)  # Use the same batch size\n",
    "\n",
    "# Make predictions on the test dataset\n",
    "y_pred = saved_model.predict(X_test_tokens.data)\n",
    "y_pred_classes = np.argmax(y_pred.logits, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision_bert = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "recall_bert = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "f1_bert = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"BERT Model Evaluation:\")\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision_bert:.4f}\")\n",
    "print(f\"Recall: {recall_bert:.4f}\")\n",
    "print(f\"F1-Score: {f1_bert:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "report = classification_report(y_test, y_pred_classes, target_names=emotions)\n",
    "print(\"\\nClassification Report:\\n\", report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
